{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain v0.3 - Einführung in die Nutzung mit Beispielen in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain\n",
    "#pip install langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Dienste von OpenAI und TogetherAI nutzen zu können, benötigen wir einen API-Key. Nach der Registrierung bei den jeweiligen Anbietern findet man diesen Schlüssel meist im Benutzer-Dashboard (oder einem ähnlichen Bereich). Der Key ist in der Regel eine längere Sequenz aus Zahlen und Buchstaben.\n",
    "\n",
    "Um Missbrauch zu verhindern, sollten diese Keys nicht direkt im Code gespeichert werden. Stattdessen speichern wir sie als Umgebungsvariablen in einer `.env`-Datei.\n",
    "\n",
    "Um die Umgebungsvariablen aus einer `.env`-Datei in die Systemumgebung zu laden, verwenden wir die Funktion `load_dotenv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um mit OpenAI-Modellen über LangChain arbeiten zu können, müssen wir das Paket `langchain-openai` installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden nur mit Chat-Modellen arbeiten. Dafür brauchen wir die Klasse `ChatOpenAI` importieren.\n",
    "\n",
    "Weitere Informationen zur Klasse findest du [hier](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt initialisieren wir das OpenAI-Modell mit der `ChatOpenAI`-Klasse. Hier wird das Modell `gpt-4o-mini` verwendet, das ein kleineres und schnelleres Modell ist. \n",
    "\n",
    "Die Temperatur ist auf 0 gesetzt, was bedeutet, dass das Modell deterministischer antwortet und weniger zufällige Variationen in den Ausgaben erzeugt.\n",
    "\n",
    "(Der API-Schlüssel wird bereits über Umgebungsvariablen bereitgestellt und daher nicht erneut im Code spezifiziert werden muss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_OpenAI_4oMini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser erstes kurzes Beispiel ist eine Übersetzung von Englischen ins Deutsche. \n",
    "\n",
    "Um das Modell anzusprechen, nutzen wir die Methode `invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_OpenAI_4oMini.invoke(\"\"\"You are a helpful assistant that translates English to German. Please translate the user sentence.\n",
    "User sentence:\n",
    "```\n",
    "I love Digital Humanities.\n",
    "```\n",
    "\"\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output zeigt die Antwort des Modells sowie relevante Metadaten als Objekt mit mehreren Attributen, die Dictionaries enthalten:\n",
    "\n",
    "- `content`: Die eigentliche Antwort des Modells.\n",
    "- `response_metadata`: Metadaten zur Anfrage, wie die Anzahl der verwendeten Tokens, der Modellname (z. B. `gpt-4o-mini-2024-07-18`), und der Grund, warum die Ausgabe gestoppt wurde (`finish_reason='stop'`).\n",
    "- `usage_metadata`: Informationen zur Anzahl der Eingabe- und Ausgabetokens, die bei der Anfrage verwendet wurden.\n",
    "\n",
    "Dieses Objekt liefert uns also nicht nur die Antwort des Modells, sondern auch nützliche Daten zur Token-Nutzung und Modellleistung.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objekt der Klasse `<class 'langchain_core.messages.ai.AIMessage'>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der obigen Anfrage wurde der Prompt als einfacher String übergeben. Eine andere Möglichkeit ist, einen Prompt mit strukturierten Nachrichten und definierten Rollen zu verwenden.\n",
    "\n",
    "Vorteil dieses Ansatzes: **Bessere Kontextualisierung**. Durch die Definition von Rollen kann das Modell besser verstehen, in welchem Kontext es agieren soll (auch wenn dies bei einfachen Beispielen oft keinen spürbaren Unterschied macht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\", \"You are a helpful assistant that translates English to German. Please translate the user sentence.\"\n",
    "    ),\n",
    "    (\"human\", \"I love Digital Humanities.\"),\n",
    "]\n",
    "result = llm_OpenAI_4oMini.invoke(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sieht das zum Beispiel mit einem anderen Modell aus?\n",
    "\n",
    "LangChain unterstützt viele verschiedene Anbieter. Hier ist eine Liste der unterstützten Chat-Modelle: https://python.langchain.com/docs/integrations/chat/#advanced-features\n",
    "\n",
    "Wir werden mit TogetherAI weiterarbeiten. \n",
    "Liste der Modelle von TogetherAI: https://api.together.xyz/models\n",
    "\n",
    "Versuchen wir es mit Llama.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aber zunächst müssen wir das Paket `langchain-together` installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_TogehterAI_Llama3_8B = ChatTogether(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct-Turbo\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ein weiteres Beispiel für die Übersetzung ins Deutsche, diesmal mit TogetherAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_TogehterAI_Llama3_8B.invoke(\"\"\"You are a helpful assistant that translates English to German. Please translate the user sentence.\n",
    "User sentence:\n",
    "```\n",
    "I love Digital Humanities.\n",
    "```\n",
    "\"\"\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Antwort ist ähnlich, jedoch mit zusätzlichen Inhalten – einem einleitenden Satz und einer zusätzlichen Anmerkung (Note)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und jetzt mit Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\", \"You are a helpful assistant that translates English to German. Please translate the user sentence.\"\n",
    "    ),\n",
    "    (\"human\", \"I love Digital Humanities.\"),\n",
    "]\n",
    "result = llm_TogehterAI_Llama3_8B.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Antwort in dieser Variante ist kompakter und entspricht eher dem, was wir uns meistens wünschen – einer Antwort ohne Zusatzinformationen.\n",
    "\n",
    "*Randbemerkung: Für diese Präsentation ist es nicht wichtig, ob die richtige Übersetzung \"Digital Humanities\" oder \"digitale Geisteswissenschaften\" lautet. Es soll lediglich das unterschiedliche Verhalten gezeigt werden, das durch verschiedene Promptformatierungen entsteht.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben der Spezifikation der Rollen durch LangChain mit 'messages' gibt es eine weitere wichtige Regel zur Formatierung: das sogenannte Prompt-Format und die Verwendung von Tags.\n",
    "\n",
    "Verschiedene LLMs können spezifische Prompt-Formate erfordern, einschließlich spezieller Tokens oder Tags, um die Eingaben korrekt zu strukturieren.\n",
    "\n",
    "Eine korrekte Prompt-Formatierung stellt sicher, dass das LLM die Anweisungen und den Kontext richtig interpretiert. Falsch formatierte Prompts können zu unerwarteten oder falschen Ausgaben führen. Indem wir System-, Benutzer- und Assistenznachrichten klar definieren, ermöglichen wir es dem LLM, den Gesprächsfluss und seine Rolle darin besser zu verstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verschiedene Modelle (und ihre Varianten) verwenden unterschiedliche Prompt-Tags. \n",
    "\n",
    "Hier sind die Informationen für Llama 3.1: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...zuerst als einfacher Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_TogehterAI_Llama3_8B.invoke(\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant that translates English to German. Please translate the user sentence.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I love Digital Humanities.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...und noch mal mit zusätzlich mit LangChain spezialisierten Rollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\", \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant that translates English to German. Please translate the user sentence.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"\"\"I love Digital Humanities.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"),\n",
    "]\n",
    "result = llm_TogehterAI_Llama3_8B.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "Jetzt wenden wir das Gleiche auf ein realistischeres Beispiel aus den Digital Humanities an.\n",
    "\n",
    "Zusätzlich wird eine neue LangChain-Komponente eingeführt: die **Prompt Templates**. Prompt Templates helfen dabei, Prompts effizient zu verwalten und wiederzuverwenden. Durch die Verwendung von Platzhaltern innerhalb von Templates können wir Inhalte dynamisch einfügen, was unsere Prompts flexibel und anpassbar macht, ohne sie jedes Mal neu schreiben zu müssen.\n",
    "\n",
    "LangChain bietet die Klassen `PromptTemplate` und `ChatPromptTemplate` an:\n",
    "\n",
    "- `PromptTemplate` wird hauptsächlich verwendet, um einen einzelnen String zu formatieren und eignet sich für einfachere Eingaben.\n",
    "- `ChatPromptTemplate` dient zur Formatierung einer Liste von Nachrichten.\n",
    "\n",
    "Weitere Informationen gibt es hier: https://python.langchain.com/docs/concepts/#prompt-templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String PromptTemplates\n",
    "\n",
    "\n",
    "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt zwei Möglichkeiten, PromptTemplates zu initialisieren (siehe den API-Referenz-Link oben). Hier demonstrieren wir ausschließlich die von LangChain empfohlene Variante: die Initialisierung mit `from_template`.\n",
    "\n",
    "Was machen wir?\n",
    "\n",
    "1. Definition des Templates für den Prompt als Text.\n",
    "\n",
    "2. Initialisierung einer PromptTemplate-Klasse:\n",
    "    - Verwende das zuvor definierte `template_text`, um ein `PromptTemplate`-Objekt zu erstellen.\n",
    "\n",
    "3. Festlegung des historischen Textes (Regeste), der bearbeitet werden soll.\n",
    "\n",
    "4. Erstellung des finalen Prompts durch Einfügen der Regeste in das Template:\n",
    "    - Übergabe der Regeste als Eingabevariable (`question`) an das Template.\n",
    "\n",
    "5. Senden des erstellten Prompts an das Sprachmodell.\n",
    "\n",
    "6. Rückgabe des Ergebnisses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ires in ehrn und tugent wolverhaltens halben Schloss und Herrschaft Ambras sambt allem pau, haus und vorrath, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und geben zu Prag den 3. tag des monats martii nach Christi unsers herrn geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Or. Perg. mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "prompt = template.invoke({\"question\": regeste})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_TogehterAI_Llama3_8B.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatPromptTemplates\n",
    "\n",
    "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird die gleiche Aufgabe mit einer `ChatPromptTemplate` realisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Ablauf unterscheidet sich kaum von `PromptTemplate`:\n",
    "\n",
    "1. Definition der Systemnachricht (`system_msg`).\n",
    "\n",
    "2. Definition der Benutzernachricht (`user_msg`).\n",
    "\n",
    "3. Erstellung eines `ChatPromptTemplate`-Objekt.\n",
    "\n",
    "4. Festlegung des historischen Textes (Regeste), der bearbeitet werden soll.\n",
    "\n",
    "5. Erzeugung des finalen Prompts.\n",
    "\n",
    "6. Senden des erstellten Prompts an das Sprachmodell.\n",
    "\n",
    "7. Rückgabe des Ergebnisses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", system_msg),\n",
    "    (\"user\", user_msg)\n",
    "])\n",
    "\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ires in ehrn und tugent wolverhaltens halben Schloss und Herrschaft Ambras sambt allem pau, haus und vorrath, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und geben zu Prag den 3. tag des monats martii nach Christi unsers herrn geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Or. Perg. mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "prompt = prompt_template.invoke({\"question\": regeste})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{prompt.messages[0].content}{prompt.messages[1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_TogehterAI_Llama3_8B.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting\n",
    "\n",
    "Eine häufig verwendete Technik, um die Leistung des Modells zu verbessern, ist das Einbinden von Beispielen in den Prompt. Diese Methode wird als Few-Shot-Prompting bezeichnet. Durch das Bereitstellen konkreter Beispiele erhält das Sprachmodell eine klare Vorstellung davon, wie es sich verhalten soll.\n",
    "\n",
    "Um die Lesbarkeit des Codes zu verbessern, werden die Beispiele aus einer CSV-Datei eingelesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden im Prompt zwei Beispiele einfügen, die ebenfalls als Nachrichten mit definierten Rollen angegeben werden.\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "1. Öffnung der JSON-Datei mit den Beispielen und Laden des Inhalts in die Variable `examples`.\n",
    "\n",
    "2. Definition der Systemnachricht (`system_msg`).\n",
    "\n",
    "3. Definition der Benutzernachricht (`example_user_msg`) für die Beispiele.\n",
    "\n",
    "4. Definition der Assistentennachricht (`example_assistant_msg`) für die Beispiele.\n",
    "\n",
    "5. Definition der Benutzernachricht (`user_msg`) für die eigentliche Aufgabe.\n",
    "\n",
    "6. Initialisierung eines `ChatPromptTemplate`-Objekts.\n",
    "\n",
    "7. Speichern der Benutzereingaben und Assistentenantworten für Beispiel 1 und Beispiel 2 als Variablen.\n",
    "\n",
    "8. Festlegung des historischen Textes (Regeste), der analysiert werden soll.\n",
    "\n",
    "9. Erstellung des `variables`-Dictionaries:\n",
    "    - Festlegen der Platzhalter-Werte für die beiden Beispiele und den Regestetext, die in das Template eingefügt werden.\n",
    "\n",
    "10. Erzeugung des finalen Prompts.\n",
    "\n",
    "11. Senden des erstellten Prompts an das Sprachmodell.\n",
    "\n",
    "12. Rückgabe des Ergebnisses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/examples.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"Please produce statements from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", system_msg),\n",
    "    \n",
    "    # First Example\n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "    \n",
    "    # Second Example\n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "    \n",
    "    # Actual User Message\n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "example_output_1 = examples[0][\"assistant_output_text\"]\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "example_output_2 = examples[1][\"assistant_output_text\"]\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ihres in Ehren und Tugend wohlverhaltens halben Schloss und Herrschaft Ambras samt allem Bau, Haus und Vorrat, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und gegeben zu Prag den 3. Tag des Monats März nach Christi unseres Herrn Geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Originalpergament mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "variables = {\n",
    "    \"example_input_1\": example_input_1,\n",
    "    \"example_output_1\": example_output_1,\n",
    "    \"example_input_2\": example_input_2,\n",
    "    \"example_output_2\": example_output_2,\n",
    "    \"question\": regeste,\n",
    "}\n",
    "\n",
    "prompt = prompt_template.invoke(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in prompt.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_TogehterAI_Llama3_8B(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Antwort enthält jetzt weniger oder gar keinen einleitenden Text und keine Kommentare mehr. Nun wiederholen wir den Vorgang mit Beispielen im JSON-Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Examples from JSON File\n",
    "with open('data/examples.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "# System Message\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# User Message Template\n",
    "example_user_msg = \"\"\"Please produce statements from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Assistant Message Template\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Actual User Message with Placeholder\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# The ChatPromptTemplate Including All Examples\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_msg),\n",
    "    \n",
    "    # First Example\n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "    \n",
    "    # Second Example\n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "    \n",
    "    # Actual User Message\n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "# First Example Input; from examles.json\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "\n",
    "example_output_1 = examples[0][\"assistant_output_json\"]\n",
    "\n",
    "# **Third Example Input**\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "\n",
    "example_output_2 = examples[1][\"assistant_output_json\"]\n",
    "\n",
    "\n",
    "# Actual Input Text\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ihres in Ehren und Tugend wohlverhaltens halben Schloss und Herrschaft Ambras samt allem Bau, Haus und Vorrat, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und gegeben zu Prag den 3. Tag des Monats März nach Christi unseres Herrn Geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Originalpergament mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "# Variables for Placeholders\n",
    "variables = {\n",
    "    \"example_input_1\": example_input_1,\n",
    "    \"example_output_1\": example_output_1,\n",
    "    \"example_input_2\": example_input_2,\n",
    "    \"example_output_2\": example_output_2,\n",
    "    \"question\": regeste,\n",
    "}\n",
    "\n",
    "# Invoke the Prompt with the Provided Inputs\n",
    "prompt = prompt_template.invoke(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in prompt.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_TogehterAI_Llama3_8B(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output scheint im JSON-Format zu sein, doch wenn man den Objekttyp überprüft, stellt man fest, dass es sich um einen String handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wäre jedoch wünschenswert, den Output direkt als JSON-Objekt zu erhalten. Ein JSON-Objekt ermöglicht eine einfachere Datenverarbeitung: Da es sich um strukturierte Daten handelt, lassen sich JSON-Objekte leicht im Code verarbeiten. Man kann direkt auf einzelne Felder zugreifen, ohne den Text vorher parsen zu müssen.Hierfür bietet LangChain die sogenannten **Output-Parsers** an. (Immer mehr Modelle unterstützen inzwischen das sogenannte \"Function\" oder \"Tool Calling\", das diesen Prozess automatisch übernimmt und es wird empfohlen, \"Function/Tool Calling\" anstelle von Output Parsing zu verwenden, wenn dies möglich ist.)\n",
    "\n",
    "https://gorilla.cs.berkeley.edu/leaderboard.html\n",
    "\n",
    "\n",
    "Die Formatierung des Outputs werden wir gemeinsam mit einer anderen LangChain-Komponente einführen, nämlich den **Chains**. Bevor wir jedoch damit weitermachen, noch ein kurzer Seitenkommentar zu den Beispielen. In unserem Fall haben wir nur zwei Beispiele, die fest im Prompt verankert sind. Für komplexere Anwendungsfälle kann es jedoch sinnvoll sein, mehrere Beispiele zu haben und dynamisch zwei oder drei davon auszuwählen, um sie dem Prompt hinzuzufügen. LangChain hat hierfür die sogenannten **Example Selectors** eingeführt. Diese Klassen sind dafür verantwortlich, Beispiele auszuwählen und in den Prompt zu integrieren (z.B. anhand der semantischen Ähnlichkeit zwischen unserem Input und den Beispielen). Weitere Informationen findest du hier: https://python.langchain.com/docs/how_to/#example-selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains (+Output-Format)\n",
    "\n",
    "Chains sind Abfolgen von Aufgaben, bei denen das Ergebnis einer Aufgabe als Eingabe für die nächste verwendet wird. Sie automatisieren Arbeitsabläufe, verbessern die Lesbarkeit des Codes und steigern die Effizienz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "\n",
    "1. Öffnung der JSON-Datei mit den Beispielen und Laden des Inhalts in die Variable `examples`.\n",
    "\n",
    "2. Definition der Systemnachricht (`system_msg`).\n",
    "\n",
    "3. Definition der Benutzernachricht (`example_user_msg`) für die Beispiele.\n",
    "\n",
    "4. Definition der Assistentennachricht (`example_assistant_msg`) für die Beispiele.\n",
    "\n",
    "5. Definition der Benutzernachricht (`user_msg`) für die eigentliche Aufgabe.\n",
    "\n",
    "6. Initialisierung eines `ChatPromptTemplate`-Objekts.\n",
    "\n",
    "7. Speichern der Benutzereingaben und Assistentenantworten für Beispiel 1 und Beispiel 2 als Variablen.\n",
    "\n",
    "8. Festlegung des historischen Textes (Regeste), der analysiert werden soll.\n",
    "\n",
    "9. Erstellung des `variables`-Dictionaries.\n",
    "\n",
    "10. Verkettung von PromptTemplate und des Modells:\n",
    "    - Eine Kette wird erstellt, die das `prompt_template` mit dem Modell `llm_TogetherAI_Llama3_8B` verbindet.\n",
    "\n",
    "11. Ausführung der Kette mit den Variablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/examples.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"Please produce statements from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_msg),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "\n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "\n",
    "example_output_1 = examples[0][\"assistant_output_json\"]\n",
    "\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "\n",
    "example_output_2 = examples[1][\"assistant_output_json\"]\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ihres in Ehren und Tugend wohlverhaltens halben Schloss und Herrschaft Ambras samt allem Bau, Haus und Vorrat, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und gegeben zu Prag den 3. Tag des Monats März nach Christi unseres Herrn Geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Originalpergament mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "variables = {\n",
    "    \"example_input_1\": example_input_1,\n",
    "    \"example_output_1\": example_output_1,\n",
    "    \"example_input_2\": example_input_2,\n",
    "    \"example_output_2\": example_output_2,\n",
    "    \"question\": regeste,\n",
    "}\n",
    "\n",
    "# prompt = prompt_template.invoke(variables) #ALT!\n",
    "\n",
    "chain = prompt_template | llm_TogehterAI_Llama3_8B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Chain ist ein Objekt, das dazu dient, eine Abfolge von ausführbaren Aufgaben oder Aktionen darzustellen. Eine RunnableSequence definiert eine Abfolge von Operationen, die nacheinander ausgeführt werden sollen.\n",
    "\n",
    "Mehr Informationen über Runnables in LangChain findest du hier: https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.first)\n",
    "#print(chain.middle)\n",
    "#print(chain.last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.first.input_variables) \n",
    "print(chain.first.messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = llm_TogehterAI_Llama3_8B(prompt) #ALT!\n",
    "result = chain.invoke(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist etwas schwieriger, sich den tatsächlichen Prompt anzeigen zu lassen, da dieser nie explizit erstellt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in chain.first.messages:\n",
    "    print(message.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_as_string = prompt_template.invoke(variables)\n",
    "\n",
    "for message in prompt_as_string.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...aber nun zurück zu unserem Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json Output Parser\n",
    "Zunächst werden wir den einfachsten `JsonOutputParser` verwenden. Es ist wichtig zu betonen, dass der LLM-Output bereits in einer Form vorliegt, die die Aufgabe des Parsers erheblich erleichtert (dank der bereitgestellten Beispiele)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "format_instructions = parser.get_format_instructions() #Provides instructions on how the LLM should format its output, based on the expected JSON schema.\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/examples.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"Please produce statements from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", system_msg),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "    \n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "# Bind format instructions\n",
    "prompt_template = prompt_template.partial(format_instructions=format_instructions)\n",
    "\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "\n",
    "example_output_1 = examples[0][\"assistant_output_json\"]\n",
    "\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "\n",
    "example_output_2 = examples[1][\"assistant_output_json\"]\n",
    "\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ihres in Ehren und Tugend wohlverhaltens halben Schloss und Herrschaft Ambras samt allem Bau, Haus und Vorrat, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und gegeben zu Prag den 3. Tag des Monats März nach Christi unseres Herrn Geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Originalpergament mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "variables = {\n",
    "    \"example_input_1\": example_input_1,\n",
    "    \"example_output_1\": example_output_1,\n",
    "    \"example_input_2\": example_input_2,\n",
    "    \"example_output_2\": example_output_2,\n",
    "    \"question\": regeste,\n",
    "}\n",
    "\n",
    "chain = prompt_template | llm_TogehterAI_Llama3_8B | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain)\n",
    "print(\"--------------------\")\n",
    "print(chain.first.partial_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Parser\n",
    "\n",
    "Dafür werden wir den `PydanticOutputParser` verwenden. Dieser Parser basiert auf Pydantic, der am weitesten verbreiteten Bibliothek zur Datenvalidierung in Python. Mit dem `PydanticOutputParser` können wir den Output des Modells gemäß einem für unser Beispiel passenden Schema erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Statement(BaseModel):\n",
    "    text: str = Field(description=\"An informative sentence extracted from the historical text.\")\n",
    "\n",
    "class StatementGroup(BaseModel):\n",
    "    date: str = Field(description=\"The date associated with the group of statements.\")\n",
    "    statements: List[Statement] = Field(description=\"A list of statements associated with the date.\")\n",
    "\n",
    "class AssistantOutput(BaseModel):\n",
    "    statement_groups: List[StatementGroup] = Field(description=\"A list of statement groups, each associated with a date.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=AssistantOutput)\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/examples.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"Please produce statements from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", system_msg),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "    \n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "# Bind format instructions\n",
    "prompt_template = prompt_template.partial(format_instructions=format_instructions)\n",
    "\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "\n",
    "example_output_1 = examples[0][\"assistant_output_json\"]\n",
    "\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "\n",
    "example_output_2 = examples[1][\"assistant_output_json\"]\n",
    "\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ihres in Ehren und Tugend wohlverhaltens halben Schloss und Herrschaft Ambras samt allem Bau, Haus und Vorrat, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und gegeben zu Prag den 3. Tag des Monats März nach Christi unseres Herrn Geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Originalpergament mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "variables = {\n",
    "    \"example_input_1\": example_input_1,\n",
    "    \"example_output_1\": example_output_1,\n",
    "    \"example_input_2\": example_input_2,\n",
    "    \"example_output_2\": example_output_2,\n",
    "    \"question\": regeste,\n",
    "}\n",
    "\n",
    "chain = prompt_template | llm_TogehterAI_Llama3_8B | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.first.partial_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pydantic = chain.invoke(variables)\n",
    "print(result_pydantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result_pydantic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for statement_group in result_pydantic.statement_groups:\n",
    "    print(statement_group.date)\n",
    "    for statement in statement_group.statements:\n",
    "        print(statement.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the AssistantOutput Pydantic object to a dictionary\n",
    "result_dict = result_pydantic.dict()\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "json_output = json.dumps(result_dict, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_TogehterAI_Llama3_70B_Lite = ChatTogether(\n",
    "    model=\"meta-llama/Meta-Llama-3-70B-Instruct-Lite\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm_TogehterAI_Llama3_70B_Lite | parser\n",
    "result_pydantic = chain.invoke(variables)\n",
    "result_dict = result_pydantic.dict()\n",
    "json_output = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "print(json_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
