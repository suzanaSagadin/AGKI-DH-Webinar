{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verkettung von Textextraktion, NER und Namensauflösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Einrichtung der Modelle.\n",
    "2. Definition der Datenstruktur (Output).\n",
    "3. Vorbereitung von Beispielen und Anweisungen für die Modelle.\n",
    "4. Die Hauptaufgaben sind:\n",
    "    - Extrahieren von Aussagen: Aufteilung des historischen Textes in sinnvolle, zeitlich geordnete Aussagen.\n",
    "    - Durchführen von NER: Analyse jeder Aussage, um wichtige Entitäten wie Namen, Orte und Handlungen zu identifizieren.\n",
    "    - Identifikation von Personen: Mithilfe von Embeddings und Sprachmodellen die beste Übereinstimmung für jede identifizierte Entität aus einer Datenbank finden:\n",
    "        1. Alle Namen (mit IDs) in einer Vektordatenbank speichern.\n",
    "        2. Für jede Person die drei Top-Matches aus der Vektordatenbank extrahieren.\n",
    "        3. Die drei Top-Matches zusammen mit dem Personennamen und dem Kontext (Statement) an ein Sprachmodell schicken, um eine Entscheidung zu treffen.\n",
    "5. Kombination aller Aufgaben zu einer durchgehenden Prozesskette.\n",
    "6. Ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.runnables.base import RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_TogehterAI_Llama3_8B = ChatTogether(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct-Turbo\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm_TogehterAI_Llama3_70B_Lite = ChatTogether(\n",
    "    model=\"meta-llama/Meta-Llama-3-70B-Instruct-Lite\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEROutput(BaseModel):\n",
    "    Start: str = \"unknown\"\n",
    "    End: str = \"unknown\"\n",
    "    # Handlung_ausgeführt_von: Optional[str] = None\n",
    "    # Ort_der_Handlung: Optional[str] = None\n",
    "    # übertragenes_Objekt: Optional[str] = None\n",
    "    # empfänger: Optional[str] = None\n",
    "    # Art_des_Verhältnisses: Optional[str] = None\n",
    "    # Elternteil: Optional[str] = None\n",
    "    # Kind: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        extra = \"allow\"  # Allow extra fields\n",
    "\n",
    "parser_ner = PydanticOutputParser(pydantic_object=NEROutput)\n",
    "format_instructions_NER = parser_ner.get_format_instructions()\n",
    "#print(format_instructions_NER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestMatchOutput(BaseModel):\n",
    "    best_match_name: Union[str, None] = Field(\n",
    "        ..., description=\"The name of the best match, or 'None' if none are correct.\"\n",
    "    )\n",
    "\n",
    "parser_best_match = PydanticOutputParser(pydantic_object=BestMatchOutput)\n",
    "\n",
    "format_instructions_best_match = parser_best_match.get_format_instructions()\n",
    "print(format_instructions_best_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statement(BaseModel):\n",
    "    text: str = Field(description=\"An informative sentence extracted from the historical text.\")\n",
    "    ner_output: Optional[NEROutput] = None\n",
    "\n",
    "class StatementGroup(BaseModel):\n",
    "    date: str = Field(description=\"The date associated with the group of statements.\")\n",
    "    statements: List[Statement] = Field(description=\"A list of statements associated with the date.\")\n",
    "\n",
    "class AssistantOutput(BaseModel):\n",
    "    statement_groups: List[StatementGroup] = Field(description=\"A list of statement groups, each associated with a date.\")\n",
    "\n",
    "parser_statement_extraction = PydanticOutputParser(pydantic_object=AssistantOutput)\n",
    "format_instructions_statement_extraction = parser_statement_extraction.get_format_instructions()\n",
    "#print(format_instructions_statement_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/examples.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to process a provided historical record. For each given text please identify all statements that focus on the relationships and dependencies between entities in the given text, including biographical data. Also, structure the information, meaning:  Divide the text into two time based logical segments, each containing a self-contained piece of information (e.g., JJJJ-MM-TT, before JJJJ-MM-TT)\n",
    "Return the statements in german as simple sentences that allow for easy interpretation and reconstruction of the historical context. Each sentence must be informative in itself. Please ensure to include names in each sentence as demonstrated:\n",
    "John Doe goes to the store. John Doe buys apples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"Please produce statements from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"Please produce statements from the following text:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt_template_statement_extraction = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_msg),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "    \n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "    \n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "prompt_template_statement_extraction = prompt_template_statement_extraction.partial(format_instructions=format_instructions_statement_extraction)\n",
    "\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "\n",
    "example_output_1 = examples[0][\"assistant_output_json\"]\n",
    "\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "\n",
    "example_output_2 = examples[1][\"assistant_output_json\"]\n",
    "\n",
    "\n",
    "regeste = \"\"\"8652 1564 März 3, Prag.\n",
    "\n",
    "Erzherzog Ferdinand schenkt der edlen Philippina Welserin sonderlich ihres in Ehren und Tugend wohlverhaltens halben Schloss und Herrschaft Ambras samt allem Bau, Haus und Vorrat, das ihm sein Vater Kaiser Ferdinand I. geschenksweise überlassen habe.\n",
    "\n",
    "Geschehen und gegeben zu Prag den 3. Tag des Monats März nach Christi unseres Herrn Geburt im 1564.\n",
    "\n",
    "Eigenhändig unterschriebenes Originalpergament mit abgefallenem Siegel, Urkunden des Familienarchivs.\"\"\"\n",
    "\n",
    "variables_statement_extraction = {\n",
    "    \"example_input_1\": example_input_1,\n",
    "    \"example_output_1\": example_output_1,\n",
    "    \"example_input_2\": example_input_2,\n",
    "    \"example_output_2\": example_output_2,\n",
    "    \"question\": regeste,\n",
    "}\n",
    "\n",
    "statement_extraction_chain = prompt_template_statement_extraction | llm_TogehterAI_Llama3_70B_Lite | parser_statement_extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/examples_NER.json', 'r', encoding='utf-8') as file:\n",
    "    examples = json.load(file)\n",
    "\n",
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a meticulous analyst with a focus on historical data. Your task is to produce a structured output for each given text.\n",
    "\n",
    "Your goal is to extract information about interactions, activities, relationships, and biographical details to enrich our prosopographical database.<|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"Please produce a structured output describing the action from the following text:\n",
    "{example_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"\n",
    "\n",
    "Please produce a structured output describing the action from the following text:\n",
    "{statement}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_NER = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_msg),\n",
    "\n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_1}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_1}\")),\n",
    "\n",
    "    (\"user\", example_user_msg.replace(\"{example_input}\", \"{example_input_2}\")),\n",
    "    (\"assistant\", example_assistant_msg.replace(\"{example_output}\", \"{example_output_2}\")),\n",
    "\n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "prompt_template_NER = prompt_template_NER.partial(format_instructions=format_instructions_NER)\n",
    "\n",
    "example_input_1 = examples[0][\"user_input\"]\n",
    "\n",
    "example_output_1 = examples[0][\"assistant_output_json\"]\n",
    "\n",
    "example_input_2 = examples[1][\"user_input\"]\n",
    "\n",
    "example_output_2 = examples[1][\"assistant_output_json\"]\n",
    "\n",
    "chain_NER = prompt_template_NER | llm_TogehterAI_Llama3_8B | parser_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load  CSV data and create embeddings\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "df = pd.read_csv('data/names_ids.csv')\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "names = df['name'].tolist()\n",
    "ids = df['id'].tolist()\n",
    "\n",
    "metadatas = [{'id': id_, 'name': name} for id_, name in zip(ids, names)]\n",
    "\n",
    "vector_store = FAISS.from_texts(names, embedding_model, metadatas=metadatas)\n",
    "\n",
    "# Retriever (TODO: @chain decorator to the function to create a Runnable)\n",
    "def find_top_matches_with_context(name, full_statement, top_k=3):\n",
    "    # name_with_context = f\"{name} - {full_statement}\"\n",
    "    \n",
    "    results = vector_store.similarity_search_with_relevance_scores(name, k=top_k)\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    for doc, similarity_score in results:\n",
    "        # Convert np.float32 to Python float\n",
    "        python_float_score = float(similarity_score)\n",
    "        match = {\n",
    "            'score': python_float_score,\n",
    "            'name': doc.metadata.get('name', ''),\n",
    "            'id': doc.metadata['id'],\n",
    "        }\n",
    "        matches.append(match)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "You are an assistant that helps clarify name matches based on contextual information. Based on the context of the statement, return the correct match.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_msg = \"\"\"You are given the following statement: \"{statement}\"\n",
    "The name mentioned is: \"{name}\"\n",
    "\n",
    "Here are three possible matches:\n",
    "{match_1}\n",
    "{match_2}\n",
    "{match_3}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "example_user_msg = \"\"\"You are given the following statement: Maximilian befiehlt Johannes Ried das 'Riesenbuch' zu schreiben.\n",
    "The name mentioned is: Maximilian\n",
    "\n",
    "Here are three possible matches:\n",
    "Maximilian I., römischer König und Kaiser (1486/1508-1519)\n",
    "Maximilian Prandstetter, Bürger und Umgelter zu Linz an der Donau\n",
    "Maximilian Stürtzel, Kleriker, Sohn von Konrad Stürtzel dem Älteren<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "example_assistant_msg = \"\"\"{example_output}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_RAG = ChatPromptTemplate([\n",
    "    (\"system\", system_msg),\n",
    "\n",
    "    (\"user\", example_user_msg),\n",
    "    (\"assistant\", example_assistant_msg),\n",
    "\n",
    "    (\"user\", user_msg),\n",
    "])\n",
    "\n",
    "prompt_template_RAG = prompt_template_RAG.partial(format_instructions=format_instructions_best_match)\n",
    "\n",
    "example_output_best_match = \"\"\"\n",
    "{\"best_match_name\": \"Maximilian I., römischer König und Kaiser (1486/1508-1519)\"}\n",
    "\"\"\"\n",
    "\n",
    "decide_best_match_chain = prompt_template_RAG | llm_TogehterAI_Llama3_70B_Lite | parser_best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send top matches to LLM for final decision\n",
    "def decide_best_match_via_llm(name, statement, top_matches):\n",
    "\n",
    "    variables_statement_RAG = {\n",
    "    \"statement\": statement,\n",
    "    \"name\": name,\n",
    "    \"match_1\": top_matches[0]['name'],\n",
    "    \"match_2\": top_matches[1]['name'],\n",
    "    \"match_3\": top_matches[2]['name'],\n",
    "    \"example_output\": example_output_best_match,\n",
    "    }\n",
    "\n",
    "    result = decide_best_match_chain.invoke(variables_statement_RAG)\n",
    "\n",
    "    return result.best_match_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform NER on a single statement\n",
    "def ner_on_statement(result_dict):\n",
    "    for statement_group in result_dict.statement_groups:\n",
    "        for statement in statement_group.statements:\n",
    "            statement_variables = {\n",
    "                \"example_input_1\": example_input_1,\n",
    "                \"example_output_1\": example_output_1,\n",
    "                \"example_input_2\": example_input_2,\n",
    "                \"example_output_2\": example_output_2,\n",
    "                \"statement\": statement.text,\n",
    "            }\n",
    "            ner_output = chain_NER.invoke(statement_variables)\n",
    "            statement.ner_output = ner_output\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_on_ner_output(result_dict):\n",
    "    name_fields = ['Handlung_ausgeführt_von', 'empfänger', 'Elternteil', 'Kind', 'Person']\n",
    "    for statement_group in result_dict.statement_groups:\n",
    "        for statement in statement_group.statements:\n",
    "            ner_output = statement.ner_output\n",
    "            for field in name_fields:\n",
    "                name = getattr(ner_output, field, None)\n",
    "                if name:\n",
    "                    top_matches = find_top_matches_with_context(name, statement.text)\n",
    "                    setattr(ner_output, f'{field}_matches', top_matches)\n",
    "                    best_match_name = decide_best_match_via_llm(name, statement.text, top_matches)\n",
    "                    setattr(ner_output, f'{field}_best_match', best_match_name)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_process_statements = RunnableLambda(ner_on_statement)\n",
    "runnable_best_match = RunnableLambda(rag_on_ner_output)\n",
    "\n",
    "# Combine the chains into a final chain\n",
    "final_chain = statement_extraction_chain | runnable_process_statements | runnable_best_match\n",
    "\n",
    "result = final_chain.invoke(variables_statement_extraction)\n",
    "\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = result.dict()\n",
    "json_output = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "print(json_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
